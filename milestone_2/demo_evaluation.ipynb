{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Setup\n",
    "\n",
    "In the setup, we will import the necessary libraries and define the queries that we will use to retrieve the documents from the Solr instance. With the queries, we will also define the corresponding relevance judgements."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f287e5d45b646b7c"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "import numpy as np\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T15:24:09.156577Z",
     "start_time": "2023-10-26T15:24:09.148021Z"
    }
   },
   "id": "97233b62f81b92cb"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mJSONDecodeError\u001B[0m                           Traceback (most recent call last)",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/requests/models.py:971\u001B[0m, in \u001B[0;36mResponse.json\u001B[0;34m(self, **kwargs)\u001B[0m\n\u001B[1;32m    970\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 971\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcomplexjson\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloads\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    972\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m JSONDecodeError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    973\u001B[0m     \u001B[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001B[39;00m\n\u001B[1;32m    974\u001B[0m     \u001B[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001B[39;00m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/__init__.py:346\u001B[0m, in \u001B[0;36mloads\u001B[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001B[0m\n\u001B[1;32m    343\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\u001B[38;5;28mcls\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m object_hook \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m\n\u001B[1;32m    344\u001B[0m         parse_int \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m parse_float \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m\n\u001B[1;32m    345\u001B[0m         parse_constant \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m object_pairs_hook \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m kw):\n\u001B[0;32m--> 346\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_default_decoder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43ms\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    347\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mcls\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/decoder.py:337\u001B[0m, in \u001B[0;36mJSONDecoder.decode\u001B[0;34m(self, s, _w)\u001B[0m\n\u001B[1;32m    333\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001B[39;00m\n\u001B[1;32m    334\u001B[0m \u001B[38;5;124;03mcontaining a JSON document).\u001B[39;00m\n\u001B[1;32m    335\u001B[0m \n\u001B[1;32m    336\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m--> 337\u001B[0m obj, end \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mraw_decode\u001B[49m\u001B[43m(\u001B[49m\u001B[43ms\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43midx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_w\u001B[49m\u001B[43m(\u001B[49m\u001B[43ms\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mend\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    338\u001B[0m end \u001B[38;5;241m=\u001B[39m _w(s, end)\u001B[38;5;241m.\u001B[39mend()\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/decoder.py:355\u001B[0m, in \u001B[0;36mJSONDecoder.raw_decode\u001B[0;34m(self, s, idx)\u001B[0m\n\u001B[1;32m    354\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[0;32m--> 355\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m JSONDecodeError(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpecting value\u001B[39m\u001B[38;5;124m\"\u001B[39m, s, err\u001B[38;5;241m.\u001B[39mvalue) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    356\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m obj, end\n",
      "\u001B[0;31mJSONDecodeError\u001B[0m: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mJSONDecodeError\u001B[0m                           Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[18], line 7\u001B[0m\n\u001B[1;32m      5\u001B[0m relevant \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mmap\u001B[39m(\u001B[38;5;28;01mlambda\u001B[39;00m el: el\u001B[38;5;241m.\u001B[39mstrip(), \u001B[38;5;28mopen\u001B[39m(QRELS_FILE)\u001B[38;5;241m.\u001B[39mreadlines()))\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# Get query results from Solr instance\u001B[39;00m\n\u001B[0;32m----> 7\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[43mrequests\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mQUERY_URL\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjson\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mresponse\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdocs\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/requests/models.py:975\u001B[0m, in \u001B[0;36mResponse.json\u001B[0;34m(self, **kwargs)\u001B[0m\n\u001B[1;32m    971\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m complexjson\u001B[38;5;241m.\u001B[39mloads(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtext, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    972\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m JSONDecodeError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    973\u001B[0m     \u001B[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001B[39;00m\n\u001B[1;32m    974\u001B[0m     \u001B[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001B[39;00m\n\u001B[0;32m--> 975\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m RequestsJSONDecodeError(e\u001B[38;5;241m.\u001B[39mmsg, e\u001B[38;5;241m.\u001B[39mdoc, e\u001B[38;5;241m.\u001B[39mpos)\n",
      "\u001B[0;31mJSONDecodeError\u001B[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "QRELS_FILE = \"information_systems_qrels.txt\"\n",
    "QUERY_URL = \"http://localhost:8983/solr/#/courses/query?q=*:*&q.op=OR&indent=true&useParams=rows,sort\"\n",
    "\n",
    "# Read qrels to extract relevant documents\n",
    "relevant = list(map(lambda el: el.strip(), open(QRELS_FILE).readlines()))\n",
    "# Get query results from Solr instance\n",
    "results = requests.get(QUERY_URL).json()['response']['docs']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T15:24:09.202958Z",
     "start_time": "2023-10-26T15:24:09.152826Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Getting the Metrics table\n",
    "\n",
    "In this part, we calculate some common evaluation metrics that can be used to compare systems. We will calculate the Average Precision and Precision at 10 (P@10) metrics. The Average Precision is calculated as the average of the precision values at each relevant document; And, the P@10 is calculated as the proportion of relevant documents in the top 10 results."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ec5a368b9095764"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define custom decorator to automatically calculate metric based on key\n",
    "metrics = {}\n",
    "metric = lambda f: metrics.setdefault(f.__name__, f)\n",
    "\n",
    "# Average precision\n",
    "@metric\n",
    "def ap(results, relevant):\n",
    "    precision_values = []\n",
    "    relevant_count = 0\n",
    "\n",
    "    for idx, doc in enumerate(results):\n",
    "        if doc['id'] in relevant:\n",
    "            relevant_count += 1\n",
    "            precision_at_k = relevant_count / (idx + 1)\n",
    "            precision_values.append(precision_at_k)\n",
    "\n",
    "    if not precision_values:\n",
    "        return 0.0\n",
    "\n",
    "    return sum(precision_values)/len(precision_values)\n",
    "\n",
    "# Precision at 10 (It is at N, but we defined n=10)\n",
    "@metric\n",
    "def p10(results, relevant, n=10):\n",
    "    return len([doc for doc in results[:n] if doc['id'] in relevant])/n\n",
    "\n",
    "def calculate_metric(key, results, relevant):\n",
    "    return metrics[key](results, relevant)\n",
    "\n",
    "# Define metrics to be calculated\n",
    "evaluation_metrics = {\n",
    "    'ap': 'Average Precision',\n",
    "    'p10': 'Precision at 10 (P@10)'\n",
    "}\n",
    "\n",
    "# Calculate all metrics and export results as LaTeX table\n",
    "df = pd.DataFrame([['Metric','Value']] +\n",
    "    [\n",
    "        [evaluation_metrics[m], calculate_metric(m, results, relevant)]\n",
    "        for m in evaluation_metrics\n",
    "    ]\n",
    ")\n",
    "\n",
    "with open('results.tex','w') as tf:\n",
    "    tf.write(df.to_latex())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-26T15:24:09.202671Z"
    }
   },
   "id": "6ba670d4fde813a1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Getting the Precision-Recall Curve\n",
    "\n",
    "To finish the evaluation, we calculate a precision-recall curve, to provide a more visual clue about the systems' performance. The precision-recall curve is calculated using the precision and recall values as we move down the ranked list of documents. The precision is calculated as the proportion of relevant documents in the top N results; And, the recall is calculated as the proportion of relevant documents in the whole set of relevant documents."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "298626b63ac815a7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Calculate precision and recall values as we move down the ranked list\n",
    "precision_values = [\n",
    "    len([\n",
    "        doc \n",
    "        for doc in results[:idx]\n",
    "        if doc['id'] in relevant\n",
    "    ]) / idx \n",
    "    for idx, _ in enumerate(results, start=1)\n",
    "]\n",
    "\n",
    "recall_values = [\n",
    "    len([\n",
    "        doc for doc in results[:idx]\n",
    "        if doc['id'] in relevant\n",
    "    ]) / len(relevant)\n",
    "    for idx, _ in enumerate(results, start=1)\n",
    "]\n",
    "\n",
    "precision_recall_match = {k: v for k,v in zip(recall_values, precision_values)}\n",
    "\n",
    "# Extend recall_values to include traditional steps for a better curve (0.1, 0.2 ...)\n",
    "recall_values.extend([step for step in np.arange(0.1, 1.1, 0.1) if step not in recall_values])\n",
    "recall_values = sorted(set(recall_values))\n",
    "\n",
    "# Extend matching dict to include these new intermediate steps\n",
    "for idx, step in enumerate(recall_values):\n",
    "    if step not in precision_recall_match:\n",
    "        if recall_values[idx-1] in precision_recall_match:\n",
    "            precision_recall_match[step] = precision_recall_match[recall_values[idx-1]]\n",
    "        else:\n",
    "            precision_recall_match[step] = precision_recall_match[recall_values[idx+1]]\n",
    "\n",
    "disp = PrecisionRecallDisplay([precision_recall_match.get(r) for r in recall_values], recall_values)\n",
    "disp.plot()\n",
    "plt.savefig('precision_recall.pdf')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-26T15:24:09.203965Z"
    }
   },
   "id": "b61ebe7450d8630a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
